{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Chr Y Full Haplogroups in UKBioBank PD Cases and Controls and PD Proxies and Controls\n",
    "- **Author(s)** - Frank Grenn\n",
    "- **Date Started** - June 2021\n",
    "- **Quick Description:** logistic regression for full haplogroups with UKBB data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import scipy.stats as ss\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WRKDIR = \"$PATH/chrY\"\n",
    "BFILEDIR = f\"{WRKDIR}/y_ukbb\"\n",
    "OUTDIR = f\"{WRKDIR}/output_ukbb\"\n",
    "CARDDIR = \"$PATH/CARD\"\n",
    "#count cutoff for testing\n",
    "cutoff = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get Data and Subset Cases, Controls and Proxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fam = pd.read_csv(f\"{BFILEDIR}/chrY_eur_male_only.fam\",sep=\"\\s+\",header=None)\n",
    "fam.columns = ['fid','iid','pid','mid','sex','pheno']\n",
    "print(fam.shape)\n",
    "print(fam.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fam.pheno.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_cc_pcs = pd.read_csv(f\"{CARDDIR}/projects/chromosome_y_expression/ukbb/pcs_case_control_pca.txt\",sep=\"\\s+\")\n",
    "print(auto_cc_pcs.shape)\n",
    "print(auto_cc_pcs.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_pc_pcs = pd.read_csv(f\"{CARDDIR}/projects/chromosome_y_expression/ukbb/pcs_proxy_control_pca.txt\",sep=\"\\s+\")\n",
    "print(auto_pc_pcs.shape)\n",
    "print(auto_pc_pcs.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = pd.read_table(f\"{CARDDIR}/UKBIOBANK/PHENOTYPE_DATA/covariates_phenome_to_use.txt\")\n",
    "print(meta.shape)\n",
    "print(meta.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge\n",
    "merge1 = pd.merge(left = fam[['fid','iid','sex','pheno']], right = auto_cc_pcs, left_on = ['fid','iid'], right_on = ['FID','IID'])\n",
    "print(merge1.shape)\n",
    "merge2 = pd.merge(left = merge1, right = meta[['FID','AGE_OF_RECRUIT']], left_on = ['fid'], right_on = ['FID'])\n",
    "print(merge2.shape)\n",
    "case_control_df = merge2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge\n",
    "merge1 = pd.merge(left = fam[['fid','iid','sex','pheno']], right = auto_pc_pcs, left_on = ['fid','iid'], right_on = ['FID','IID'])\n",
    "print(merge1.shape)\n",
    "merge2 = pd.merge(left = merge1, right = meta[['FID','AGE_OF_RECRUIT']], left_on = ['fid'], right_on = ['FID'])\n",
    "print(merge2.shape)\n",
    "proxy_control_df = merge2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_control_df.pheno.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy_control_df.pheno.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\",\".join(case_control_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yhaplo Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhaplo = pd.read_csv(f\"{OUTDIR}/yhaplo_output/haplogroups.chrY_male_only.txt\",sep=\"\\s+\",header=None)\n",
    "yhaplo.columns = ['id','haplo_short','haplo_short_rep_snp','haplo_long']\n",
    "yhaplo['haplo_major'] = yhaplo['haplo_long'].str[0]\n",
    "yhaplo['id'] = [i[:len(i)//2] for i in yhaplo.id]\n",
    "yhaplo['id'] = yhaplo['id'].astype('int64')\n",
    "#assume samples with \"A\" haplogroup were not assigned one.\n",
    "#yhaplo = yhaplo[yhaplo['haplo_long']!='A']\n",
    "print(yhaplo.shape)\n",
    "print(yhaplo.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Snappy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snappy = pd.read_csv(f\"{OUTDIR}/chrY_hgs_snappy.out\",sep=\"\\t\",header=None)\n",
    "snappy.columns = ['id','haplo','haplo_score','info_alleles']\n",
    "\n",
    "#some samples, like \"PD-PDNZ095VCJ\" have extra data in the \"haplo\" column, like \"B2a1a M109,M152/Page60,P32,P50\", and we only want the \"B2a1a\"\n",
    "snappy['haplo']= snappy['haplo'].str.split(\" \").str[0]\n",
    "\n",
    "print(snappy.shape)\n",
    "print(snappy.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snappy['haplo_major'] = snappy['haplo'].str[0]\n",
    "print(snappy.shape)\n",
    "print(snappy.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Y-LineageTracker Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ltrack = pd.read_table(f\"{AMPPD_OUT}/output_ltracker/ltrack_out.hapresult.hg\",sep=\"\\s+\")\n",
    "ltrack = pd.read_table(f\"{OUTDIR}/ltrack_ukbb_hg19.lineageresult.txt\")\n",
    "print(ltrack.shape)\n",
    "print(ltrack.head())\n",
    "\n",
    "\n",
    "ltrack['haplo_major'] = ltrack['Haplogroup'].str[0]#ltrack_male['Haplogroup'].str[0]\n",
    "ltrack.columns = ['id','haplo','keyhaplo','mutations','lineagetrack','haplo_major']\n",
    "\n",
    "#ltrack['id'] = ltrack['id'].str.split('_').str[0].astype(int)\n",
    "\n",
    "ltrack['id'] = [i[:len(i)//2] for i in ltrack.id]\n",
    "ltrack['id'] = ltrack['id'].astype('int64')\n",
    "print(ltrack.shape)\n",
    "print(ltrack.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup some stats functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chi squared test for a specific haplotype\n",
    "def chi_square_for_haplogroup(haplo,haplo_col,df,prnt):\n",
    "    data = df.copy()\n",
    "    data.loc[data[haplo_col] != haplo,haplo_col] = 'not '+haplo\n",
    "\n",
    "    contingency_table = pd.crosstab(data[haplo_col], data['pheno'], margins = False) \n",
    "\n",
    "\n",
    "\n",
    "    g, p, dof, expctd = ss.chi2_contingency(contingency_table)\n",
    "    if prnt:\n",
    "        print(contingency_table)\n",
    "        print(g)\n",
    "        print(p)\n",
    "        print(dof)\n",
    "        print(expctd)\n",
    "        \n",
    "    return g, p, dof, expctd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic regression for specific full haplogroup\n",
    "def log_reg_for_haplogroup(haplo,df,prnt):\n",
    "    \n",
    "    \n",
    "    model = sm.GLM.from_formula(f\"pheno ~ {haplo} + AGE_OF_RECRUIT + PC1 + PC2 + PC3 + PC4 + PC5\",family = sm.families.Binomial(), data = df)\n",
    "    #model = sm.GLM.from_formula(f\"pheno ~ {haplo}\", data = data_no_gc_no_unknown)\n",
    "    results = model.fit()\n",
    "    if prnt:\n",
    "        print(results.summary())\n",
    "    results.summary()\n",
    "    \n",
    "    return results.pvalues[f'{haplo}'], results.params[haplo], results.bse[haplo]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to count all haplogroups, do chi squared and logistic regression for each, and return a dataframe\n",
    "def run_chisq_and_log_reg(df,haplo_col_str, count_cutoff, prnt):\n",
    "    data_crosstab = pd.crosstab(df[haplo_col_str], df['pheno'], margins = False)\n",
    "    data_crosstab.columns = ['control','case']\n",
    "    data_crosstab_filter = data_crosstab.copy()\n",
    "    if(cutoff!=0):\n",
    "        data_crosstab_filter = data_crosstab[data_crosstab.control+data_crosstab.case>=count_cutoff]\n",
    "        \n",
    "    haplos = set(data_crosstab_filter.index.tolist())\n",
    "\n",
    "    #chi squared\n",
    "    chisq_results = data_crosstab_filter.copy()\n",
    "    chisq_results['p_chisq'] = 0.1\n",
    "    for h in haplos:\n",
    "        g, p, dof, expctd = chi_square_for_haplogroup(h,haplo_col_str,df,False)\n",
    "        chisq_results.at[h,'p_chisq'] = p\n",
    "    chisq_results.columns = ['controls','cases','p_chisq']\n",
    "    chisq_results = chisq_results.reset_index()\n",
    "\n",
    "\n",
    "    #logistic regression\n",
    "    df_ohe = df.copy()\n",
    "    df_ohe[haplo_col_str+'_orig'] = df_ohe[haplo_col_str]\n",
    "    df_ohe = pd.get_dummies(df_ohe, columns = [haplo_col_str])\n",
    "    df_ohe.pheno = df_ohe.pheno - 1\n",
    "\n",
    "\n",
    "\n",
    "    logreg_results = data_crosstab_filter.copy()\n",
    "    logreg_results['p_logreg'] = 0.1\n",
    "    for h in haplos:\n",
    "        p, beta,se = log_reg_for_haplogroup(f'{haplo_col_str}_{h}',df_ohe,False)\n",
    "        logreg_results.at[h,'p_logreg'] = p\n",
    "        logreg_results.at[h,'beta_logreg'] = beta\n",
    "        logreg_results.at[h,'se_logreg'] = se\n",
    "    logreg_results.columns = ['controls','cases','p_logreg','beta_logreg','se_logreg']\n",
    "    logreg_results = logreg_results.reset_index()\n",
    "\n",
    "\n",
    "    merge_results = pd.merge(left = logreg_results, right = chisq_results,left_on = [haplo_col_str,'controls','cases'], right_on = [haplo_col_str,'controls','cases'])\n",
    "\n",
    "    merge_results['case_freq'] = merge_results['cases'] / data_crosstab['case'].sum()\n",
    "    merge_results['control_freq'] = merge_results['controls'] / data_crosstab['control'].sum()\n",
    "\n",
    "    merge_results = merge_results[[haplo_col_str,'controls','control_freq','cases','case_freq','p_chisq','p_logreg','beta_logreg','se_logreg']]\n",
    "\n",
    "\n",
    "    if(prnt):\n",
    "        print(data_crosstab)\n",
    "        print(data_crosstab_filter.shape)\n",
    "        print(data_crosstab_filter)\n",
    "        print(len(haplos))\n",
    "        print(chisq_results)\n",
    "        print(df_ohe.shape)\n",
    "        print(df_ohe.columns)\n",
    "        print(logreg_results)\n",
    "        print(merge_results.shape)\n",
    "        \n",
    "    return merge_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to count all haplogroups, do chi squared and logistic regression for each, and return a dataframe\n",
    "def run_chisq_and_log_reg_proxy_control(df,haplo_col_str, count_cutoff, prnt):\n",
    "    data_crosstab = pd.crosstab(df[haplo_col_str], df['pheno'], margins = False)\n",
    "    data_crosstab.columns = ['control','proxy']\n",
    "    data_crosstab_filter = data_crosstab.copy()\n",
    "    if(cutoff!=0):\n",
    "        data_crosstab_filter = data_crosstab[data_crosstab.control+data_crosstab.proxy>=count_cutoff]\n",
    "        \n",
    "    haplos = set(data_crosstab_filter.index.tolist())\n",
    "\n",
    "    #chi squared\n",
    "    chisq_results = data_crosstab_filter.copy()\n",
    "    chisq_results['p_chisq'] = 0.1\n",
    "    for h in haplos:\n",
    "        g, p, dof, expctd = chi_square_for_haplogroup(h,haplo_col_str,df,False)\n",
    "        chisq_results.at[h,'p_chisq'] = p\n",
    "    chisq_results.columns = ['controls','proxies','p_chisq']\n",
    "    chisq_results = chisq_results.reset_index()\n",
    "\n",
    "\n",
    "    #logistic regression\n",
    "    df_ohe = df.copy()\n",
    "    df_ohe[haplo_col_str+'_orig'] = df_ohe[haplo_col_str]\n",
    "    df_ohe = pd.get_dummies(df_ohe, columns = [haplo_col_str])\n",
    "    df_ohe.pheno = df_ohe.pheno - 1\n",
    "\n",
    "\n",
    "\n",
    "    logreg_results = data_crosstab_filter.copy()\n",
    "    logreg_results['p_logreg'] = 0.1\n",
    "    for h in haplos:\n",
    "        p, beta,se = log_reg_for_haplogroup(f'{haplo_col_str}_{h}',df_ohe,False)\n",
    "        logreg_results.at[h,'p_logreg'] = p\n",
    "        logreg_results.at[h,'beta_logreg'] = beta\n",
    "        logreg_results.at[h,'se_logreg'] = se\n",
    "    logreg_results.columns = ['controls','proxies','p_logreg','beta_logreg','se_logreg']\n",
    "    logreg_results = logreg_results.reset_index()\n",
    "\n",
    "\n",
    "    merge_results = pd.merge(left = logreg_results, right = chisq_results,left_on = [haplo_col_str,'controls','proxies'], right_on = [haplo_col_str,'controls','proxies'])\n",
    "\n",
    "    merge_results['proxy_freq'] = merge_results['proxies'] / data_crosstab['proxy'].sum()\n",
    "    merge_results['control_freq'] = merge_results['controls'] / data_crosstab['control'].sum()\n",
    "\n",
    "    merge_results = merge_results[[haplo_col_str,'controls','control_freq','proxies','proxy_freq','p_chisq','p_logreg','beta_logreg','se_logreg']]\n",
    "\n",
    "\n",
    "    if(prnt):\n",
    "        print(data_crosstab)\n",
    "        print(data_crosstab_filter.shape)\n",
    "        print(data_crosstab_filter)\n",
    "        print(len(haplos))\n",
    "        print(chisq_results)\n",
    "        print(df_ohe.shape)\n",
    "        print(df_ohe.columns)\n",
    "        print(logreg_results)\n",
    "        print(merge_results.shape)\n",
    "        \n",
    "    return merge_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Case Control Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#yhaplo\n",
    "yhaplo_cc = pd.merge(left = yhaplo, right = case_control_df, left_on = 'id', right_on = 'fid')\n",
    "print(yhaplo_cc.shape)\n",
    "print(yhaplo_cc.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhaplo_cc_cutoff_results = run_chisq_and_log_reg(yhaplo_cc,'haplo_long', cutoff, False)\n",
    "yhaplo_cc_cutoff_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#snappy\n",
    "snappy_cc = pd.merge(left = snappy, right = case_control_df, left_on = 'id', right_on = 'fid')\n",
    "print(snappy_cc.shape)\n",
    "print(snappy_cc.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a column with ~ replaced with another character to get formulas to work\n",
    "snappy_cc['haplo_no_tilde'] = snappy_cc['haplo'].str.replace('~','_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snappy_cc_cutoff_results = run_chisq_and_log_reg(snappy_cc,'haplo_no_tilde', cutoff, False)\n",
    "snappy_cc_cutoff_results['haplo'] = snappy_cc_cutoff_results['haplo_no_tilde'].str.replace('_','~')\n",
    "snappy_cc_cutoff_results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y lineage tracker\n",
    "ltrack_cc = pd.merge(left = ltrack, right = case_control_df, left_on = 'id', right_on = 'fid')\n",
    "print(ltrack_cc.shape)\n",
    "print(ltrack_cc.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a column with ~ replaced with another character to get formulas to work\n",
    "ltrack_cc['haplo_no_tilde'] = ltrack_cc['haplo'].str.replace('~','_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltrack_cc_cutoff_results = run_chisq_and_log_reg(ltrack_cc,'haplo_no_tilde', cutoff, False)\n",
    "ltrack_cc_cutoff_results['haplo'] = ltrack_cc_cutoff_results['haplo_no_tilde'].str.replace('_','~')\n",
    "ltrack_cc_cutoff_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine and Compare Tool Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snappy_cc_cutoff_results.columns = ['snappy_'+ c for c in snappy_cc_cutoff_results.columns]\n",
    "print(snappy_cc_cutoff_results.shape)\n",
    "print(snappy_cc_cutoff_results.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "yhaplo_cc_cutoff_results.columns = ['yhaplo_'+ c for c in yhaplo_cc_cutoff_results.columns]\n",
    "print(yhaplo_cc_cutoff_results.shape)\n",
    "print(yhaplo_cc_cutoff_results.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltrack_cc_cutoff_results.columns = ['ltrack_'+ c for c in ltrack_cc_cutoff_results.columns]\n",
    "print(ltrack_cc_cutoff_results.shape)\n",
    "print(ltrack_cc_cutoff_results.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_cc_haplos = list(set(snappy_cc_cutoff_results.snappy_haplo.tolist() + yhaplo_cc_cutoff_results.yhaplo_haplo_long.tolist() + ltrack_cc_cutoff_results.ltrack_haplo.tolist()))\n",
    "print(res_cc_haplos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(res_cc_haplos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_cc = pd.DataFrame(data={'haplo':res_cc_haplos})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_cc = pd.merge(left = merge_cc, right = yhaplo_cc_cutoff_results, left_on = 'haplo', right_on = 'yhaplo_haplo_long', how = 'outer')\n",
    "merge_cc = pd.merge(left = merge_cc, right = snappy_cc_cutoff_results, left_on = 'haplo', right_on = 'snappy_haplo', how = 'outer')\n",
    "\n",
    "merge_cc = pd.merge(left = merge_cc, right = ltrack_cc_cutoff_results, left_on = 'haplo', right_on = 'ltrack_haplo', how = 'outer')\n",
    "print(merge_cc.shape)\n",
    "print(merge_cc.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_cc.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_cc[(merge_cc.snappy_p_logreg<0.05) | (merge_cc.ltrack_p_logreg<0.05) | (merge_cc.yhaplo_p_logreg<0.05)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_cc.to_csv(f\"{OUTDIR}/haplotype_full_pd_case_control_cutoff_50_new.csv\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Proxy Control Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy_control_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#yhaplo\n",
    "yhaplo_pc = pd.merge(left = yhaplo, right = proxy_control_df, left_on = 'id', right_on = 'fid')\n",
    "print(yhaplo_pc.shape)\n",
    "print(yhaplo_pc.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhaplo_pc_cutoff_results = run_chisq_and_log_reg_proxy_control(yhaplo_pc,'haplo_long', cutoff, False)\n",
    "yhaplo_pc_cutoff_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#snappy\n",
    "snappy_pc = pd.merge(left = snappy, right = proxy_control_df, left_on = 'id', right_on = 'fid')\n",
    "print(snappy_pc.shape)\n",
    "print(snappy_pc.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a column with ~ replaced with another character to get formulas to work\n",
    "snappy_pc['haplo_no_tilde'] = snappy_pc['haplo'].str.replace('~','_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snappy_pc_cutoff_results = run_chisq_and_log_reg_proxy_control(snappy_pc,'haplo_no_tilde', cutoff, False)\n",
    "snappy_pc_cutoff_results['haplo'] = snappy_pc_cutoff_results['haplo_no_tilde'].str.replace('_','~')\n",
    "snappy_pc_cutoff_results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y lineage tracker\n",
    "ltrack_pc = pd.merge(left = ltrack, right = proxy_control_df, left_on = 'id', right_on = 'fid')\n",
    "print(ltrack_pc.shape)\n",
    "print(ltrack_pc.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a column with ~ replaced with another character to get formulas to work\n",
    "ltrack_pc['haplo_no_tilde'] = ltrack_pc['haplo'].str.replace('~','_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltrack_pc_cutoff_results = run_chisq_and_log_reg_proxy_control(ltrack_pc,'haplo_no_tilde', cutoff, False)\n",
    "ltrack_pc_cutoff_results['haplo'] = ltrack_pc_cutoff_results['haplo_no_tilde'].str.replace('_','~')\n",
    "ltrack_pc_cutoff_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snappy_pc_cutoff_results.columns = ['snappy_'+ c for c in snappy_pc_cutoff_results.columns]\n",
    "print(snappy_pc_cutoff_results.shape)\n",
    "print(snappy_pc_cutoff_results.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "yhaplo_pc_cutoff_results.columns = ['yhaplo_'+ c for c in yhaplo_pc_cutoff_results.columns]\n",
    "print(yhaplo_pc_cutoff_results.shape)\n",
    "print(yhaplo_pc_cutoff_results.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltrack_pc_cutoff_results.columns = ['ltrack_'+ c for c in ltrack_pc_cutoff_results.columns]\n",
    "print(ltrack_pc_cutoff_results.shape)\n",
    "print(ltrack_pc_cutoff_results.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_pc_haplos = list(set(snappy_pc_cutoff_results.snappy_haplo.tolist() + yhaplo_pc_cutoff_results.yhaplo_haplo_long.tolist() + ltrack_pc_cutoff_results.ltrack_haplo.tolist()))\n",
    "print(res_pc_haplos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(res_pc_haplos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_pc = pd.DataFrame(data={'haplo':res_pc_haplos})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_pc = pd.merge(left = merge_pc, right = yhaplo_pc_cutoff_results, left_on = 'haplo', right_on = 'yhaplo_haplo_long', how = 'outer')\n",
    "merge_pc = pd.merge(left = merge_pc, right = snappy_pc_cutoff_results, left_on = 'haplo', right_on = 'snappy_haplo', how = 'outer')\n",
    "\n",
    "merge_pc = pd.merge(left = merge_pc, right = ltrack_pc_cutoff_results, left_on = 'haplo', right_on = 'ltrack_haplo', how = 'outer')\n",
    "print(merge_pc.shape)\n",
    "print(merge_pc.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_pc.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_pc[(merge_pc.snappy_p_logreg<0.05) | (merge_pc.ltrack_p_logreg<0.05) | (merge_pc.yhaplo_p_logreg<0.05)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_pc.to_csv(f\"{OUTDIR}/haplotype_full_pd_proxy_control_cutoff_50_new.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python/3.6",
   "language": "python",
   "name": "py3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
