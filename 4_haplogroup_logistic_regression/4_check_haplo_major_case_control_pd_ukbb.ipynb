{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Chr Y Major (First Character) Haplogroups in PD Cases, Controls and Proxies Using UKBioBank Data\n",
    "- **Author(s)** - Frank Grenn\n",
    "- **Quick Description:** logistic regression for major haplogroups with UKBB data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import scipy.stats as ss\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WRKDIR = \"/PATH/chrY\"\n",
    "BFILEDIR = f\"{WRKDIR}/y_ukbb\"\n",
    "OUTDIR = f\"{WRKDIR}/output_ukbb\"\n",
    "CARDDIR = \"/PATH\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fam = pd.read_csv(f\"{BFILEDIR}/chrY_eur_male_only.fam\",sep=\"\\s+\",header=None)\n",
    "fam.columns = ['fid','iid','pid','mid','sex','pheno']\n",
    "print(fam.shape)\n",
    "print(fam.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fam.pheno.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_cc_pcs = pd.read_csv(f\"{CARDDIR}/projects/chromosome_y_expression/ukbb/pcs_case_control_pca.txt\",sep=\"\\s+\")\n",
    "print(auto_cc_pcs.shape)\n",
    "print(auto_cc_pcs.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_pc_pcs = pd.read_csv(f\"{CARDDIR}/projects/chromosome_y_expression/ukbb/pcs_proxy_control_pca.txt\",sep=\"\\s+\")\n",
    "print(auto_pc_pcs.shape)\n",
    "print(auto_pc_pcs.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = pd.read_table(f\"{CARDDIR}/UKBIOBANK/PHENOTYPE_DATA/covariates_phenome_to_use.txt\")\n",
    "print(meta.shape)\n",
    "print(meta.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhaplo = pd.read_csv(f\"{OUTDIR}/yhaplo_output/haplogroups.chrY_male_only.txt\",sep=\"\\s+\",header=None)\n",
    "yhaplo.columns = ['id','haplo_short','haplo_short_rep_snp','haplo_long']\n",
    "yhaplo['haplo_major'] = yhaplo['haplo_long'].str[0]\n",
    "yhaplo['id'] = [i[:len(i)//2] for i in yhaplo.id]\n",
    "yhaplo['id'] = yhaplo['id'].astype('int64')\n",
    "print(yhaplo.shape)\n",
    "print(yhaplo.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhaplo.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge\n",
    "merge1 = pd.merge(left = fam[['fid','iid','sex','pheno']], right = auto_cc_pcs, left_on = ['fid','iid'], right_on = ['FID','IID'])\n",
    "print(merge1.shape)\n",
    "merge2 = pd.merge(left = merge1, right = meta[['FID','AGE_OF_RECRUIT']], left_on = ['fid'], right_on = ['FID'])\n",
    "print(merge2.shape)\n",
    "merge3 = pd.merge(left = merge2, right = yhaplo[['id','haplo_major','haplo_long']], left_on = ['fid'], right_on = ['id'])\n",
    "print(merge3.shape)\n",
    "case_control_df = merge3.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge\n",
    "merge1 = pd.merge(left = fam[['fid','iid','sex','pheno']], right = auto_pc_pcs, left_on = ['fid','iid'], right_on = ['FID','IID'])\n",
    "print(merge1.shape)\n",
    "merge2 = pd.merge(left = merge1, right = meta[['FID','AGE_OF_RECRUIT']], left_on = ['fid'], right_on = ['FID'])\n",
    "print(merge2.shape)\n",
    "merge3 = pd.merge(left = merge2, right = yhaplo[['id','haplo_major','haplo_long']], left_on = ['fid'], right_on = ['id'])\n",
    "print(merge3.shape)\n",
    "proxy_control_df = merge3.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_control_df.pheno.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy_control_df.pheno.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\",\".join(case_control_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup some stats functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chi squared test for a specific haplotype\n",
    "def chi_square_for_haplogroup(haplo,df,prnt):\n",
    "    data = df.copy()\n",
    "    data.loc[data.haplo_major != haplo,'haplo_major'] = 'not '+haplo\n",
    "\n",
    "    contingency_table = pd.crosstab(data['haplo_major'], data['pheno'], margins = False) \n",
    "\n",
    "\n",
    "\n",
    "    g, p, dof, expctd = ss.chi2_contingency(contingency_table)\n",
    "    if prnt:\n",
    "        print(contingency_table)\n",
    "        print(g)\n",
    "        print(p)\n",
    "        print(dof)\n",
    "        print(expctd)\n",
    "        \n",
    "    return g, p, dof, expctd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic regression for specific major haplogroup\n",
    "def log_reg_for_haplogroup(haplo,df,prnt):\n",
    "    \n",
    "    model = sm.GLM.from_formula(f\"pheno ~ {haplo} + AGE_OF_RECRUIT + PC1 + PC2 + PC3 + PC4 + PC5\",family = sm.families.Binomial(), data = df)\n",
    "    results = model.fit()\n",
    "    if prnt:\n",
    "        print(results.summary())\n",
    "    results.summary()\n",
    "    \n",
    "    return results.pvalues[haplo], results.params[haplo], results.bse[haplo]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Case Control Tests\n",
    "use case_control_df created ealier  \n",
    "Then do chi squared tests and logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chi Squared Case Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#contingency table for chi squared later\n",
    "data_crosstab_cc = pd.crosstab(case_control_df['haplo_major'], case_control_df['pheno'], margins = False)\n",
    "data_crosstab_cc.columns = ['control','case']\n",
    "print(data_crosstab_cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the table is counting correctly\n",
    "case_control_df[(case_control_df.pheno==1)& (case_control_df.haplo_major=='R')].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### chi squared for speficic haplotypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g, p, dof, expctd = chi_square_for_haplogroup('R',case_control_df,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haplos = set(case_control_df['haplo_major'])\n",
    "haplos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_control_chisq_results = data_crosstab_cc.copy()\n",
    "case_control_chisq_results['p_chisq'] = 0.1\n",
    "for h in haplos:\n",
    "    print(h)\n",
    "    g, p, dof, expctd = chi_square_for_haplogroup(h,case_control_df,False)\n",
    "    case_control_chisq_results.at[h,'p_chisq'] = p\n",
    "case_control_chisq_results.columns = ['controls','cases','p_chisq']\n",
    "case_control_chisq_results = case_control_chisq_results.reset_index()\n",
    "print(case_control_chisq_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(case_control_chisq_results['cases'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(case_control_chisq_results['controls'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression for Case Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot encode the major haplogroups\n",
    "case_control_df_ohe = case_control_df.copy()\n",
    "case_control_df_ohe['haplo_major_orig'] = case_control_df_ohe['haplo_major']\n",
    "case_control_df_ohe = pd.get_dummies(case_control_df_ohe, columns = ['haplo_major'])\n",
    "case_control_df_ohe.pheno = case_control_df_ohe.pheno - 1\n",
    "print(case_control_df_ohe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_control_df_ohe.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_control_logreg_results = data_crosstab_cc.copy()\n",
    "case_control_logreg_results['p_logreg'] = 0.1\n",
    "case_control_logreg_results['beta_logreg'] = 0.1\n",
    "for h in haplos:\n",
    "    print(h)\n",
    "    p, beta, se = log_reg_for_haplogroup(f'haplo_major_{h}',case_control_df_ohe,False)\n",
    "    case_control_logreg_results.at[h,'p_logreg'] = p\n",
    "    case_control_logreg_results.at[h,'beta_logreg'] = beta\n",
    "    case_control_logreg_results.at[h,'se_logreg'] = se\n",
    "case_control_logreg_results.columns = ['controls','cases','p_logreg','beta_logreg','se_logreg']\n",
    "case_control_logreg_results = case_control_logreg_results.reset_index()\n",
    "print(case_control_logreg_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Case Control Data and Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_results = pd.merge(left = case_control_logreg_results, right = case_control_chisq_results,left_on = ['haplo_major','controls','cases'], right_on = ['haplo_major','controls','cases'])\n",
    "print(merge_results.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_results['case_freq'] = merge_results['cases'] / sum(merge_results['cases'])\n",
    "merge_results['control_freq'] = merge_results['controls'] / sum(merge_results['controls'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_results = merge_results[['haplo_major','controls','control_freq','cases','case_freq','p_chisq','p_logreg','beta_logreg','se_logreg']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_results.to_csv(f\"{OUTDIR}/haplotype_major_pd_case_control_newest.csv\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Proxy Control Tests\n",
    "\n",
    "use the proxy_control_df created earlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chi Squared Proxy Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#contingency table for chi squared later\n",
    "data_crosstab_pc = pd.crosstab(proxy_control_df['haplo_major'], proxy_control_df['pheno'], margins = False)\n",
    "data_crosstab_pc.columns = ['control','proxy']\n",
    "print(data_crosstab_pc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haplos = set(proxy_control_df['haplo_major'])\n",
    "haplos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy_control_chisq_results = data_crosstab_pc.copy()\n",
    "proxy_control_chisq_results['p_chisq'] = 0.1\n",
    "for h in haplos:\n",
    "    print(h)\n",
    "    g, p, dof, expctd = chi_square_for_haplogroup(h,proxy_control_df,False)\n",
    "    proxy_control_chisq_results.at[h,'p_chisq'] = p\n",
    "proxy_control_chisq_results.columns = ['controls','proxies','p_chisq']\n",
    "proxy_control_chisq_results = proxy_control_chisq_results.reset_index()\n",
    "print(proxy_control_chisq_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(proxy_control_chisq_results['proxies'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(proxy_control_chisq_results['controls'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression for Proxy Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot encode the major haplogroups\n",
    "proxy_control_df_ohe = proxy_control_df.copy()\n",
    "proxy_control_df_ohe['haplo_major_orig'] = proxy_control_df_ohe['haplo_major']\n",
    "proxy_control_df_ohe = pd.get_dummies(proxy_control_df_ohe, columns = ['haplo_major'])\n",
    "proxy_control_df_ohe.pheno = proxy_control_df_ohe.pheno - 1\n",
    "print(proxy_control_df_ohe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy_control_df_ohe.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy_control_logreg_results = data_crosstab_pc.copy()\n",
    "proxy_control_logreg_results['p_logreg'] = 0.1\n",
    "proxy_control_logreg_results['beta_logreg'] = 0.1\n",
    "for h in haplos:\n",
    "    print(h)\n",
    "    p, beta, se = log_reg_for_haplogroup(f'haplo_major_{h}',proxy_control_df_ohe,False)\n",
    "    proxy_control_logreg_results.at[h,'p_logreg'] = p\n",
    "    proxy_control_logreg_results.at[h,'beta_logreg'] = beta\n",
    "    proxy_control_logreg_results.at[h,'se_logreg'] = se\n",
    "proxy_control_logreg_results.columns = ['controls','proxies','p_logreg','beta_logreg','se_logreg']\n",
    "proxy_control_logreg_results = proxy_control_logreg_results.reset_index()\n",
    "print(proxy_control_logreg_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Proxy Control Data and Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_results = pd.merge(left = proxy_control_logreg_results, right = proxy_control_chisq_results,left_on = ['haplo_major','controls','proxies'], right_on = ['haplo_major','controls','proxies'])\n",
    "print(merge_results.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_results['proxy_freq'] = merge_results['proxies'] / sum(merge_results['proxies'])\n",
    "merge_results['control_freq'] = merge_results['controls'] / sum(merge_results['controls'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_results = merge_results[['haplo_major','controls','control_freq','proxies','proxy_freq','p_chisq','p_logreg','beta_logreg','se_logreg']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_results.to_csv(f\"{OUTDIR}/haplotype_major_pd_proxy_control_newest.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python/3.6",
   "language": "python",
   "name": "py3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
